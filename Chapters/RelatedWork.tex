\section{Literature Assessment}
\par \textit{k}-means clustering has seen hundreds of iterations over the years, judging by the number of published works you can find \cite{1}, \cite{2}, \cite{4}, \cite{7}, \cite{10}. Every iteration of the research work seeks to improve the algorithm in some way or another. It is well known that \textit{k}-means is based on achieving similar properties within the same cluster. More often that not, the property in question is a distance measure between the cluster center and the corresponding data points. This distance measure is called the mean squared distance and is usually taken to be the Euclidean distance \cite{10}. One paper \cite{30} evaluates the performance of the algorithm with other distance measures like\\

Manhattan distance:
$$ d_{xy} = \sum_{k = 1}^{n} |x_k - y_k| $$

Chebyshev distance:
$$ d_{xy} = max_k(|x_k - y_k|) $$

Minkowski distance:
$$ d_{xy} = \bigg(\sum_{k = 1}^{n} |x_k - y_k|^p \bigg)^{1/p} $$

and concludes that the best performance of the algorithm is achieved by using the Euclidean distance and the worst performance is seen with the Manhattan distance. What it fails to do is provide an accuracy score associated with each distance measure. Sure, it concludes that the Euclidean distance assists the algorithm in providing the best performance but the study fails to clearly articulate the assessment method of the algorithm. Similarly, another study successfully exhibits the performance of the algorithm using an inter-cluster distance management based on centroid estimation but it only relies on synthetic data sets for its assessment \cite{16}. This study includes three different methods for centroid estimation, viz. random selection, mean distance model and inter-cluster distance model. Overall it addresses its hypothesis fairly well and definitely takes an interesting step towards improving the efficiency of the algorithm. Initial starting conditions are mentioned as one of the most prominent issues facing \textit{k}-means clustering. A research study explores eight different initialization methods against thirty two publicly available data sets and concludes that a few of the linear time initialization methods elevate the overall performance of the \textit{k}-means algorithm while keeping its computational complexity in check \cite{7}. Several research methods also address the initialization  problem using metaheuristics and stochastic global optimization methods like simulated annealing and genetic algorithms \cite{4}, \cite{11}, \cite{12}, \cite{19}. There is a similar pattern to most of these approaches and each one tries to improve the algorithm by nuanced changes in the parameters and hyper parameters. Despite these improvements, the highest accuracy score achieved is no more than 80-85 percent. In addition to this low score, none of the methods include a distance measurement to maximize the inter-cluster distance (ICD). A couple of these papers again rely on purely synthetic data for their evaluations. Since the generation of this synthetic data relies on drawing from different distributions, it makes it harder for a fair comparison of the performance of the algorithm \cite{18}. A global \textit{k}-means algorithm approaches the problem of clustering in an incremental method and adds a cluster center at each iteration dynamically through a global search procedure \cite{4}. While the algorithm in this study is quite sound and definitely exhibits its advantages over the normal \textit{k}-means algorithm, it only reports its findings in a metric called \say{clustering error} and does not address the accuracy, sensitivity, specificity or the F1 score. This makes it harder to evaluate the overall performance of the algorithm.
\par Most of the literature research suggested that even though the \textit{k}-means algorithm was improved in some form or another, the corresponding study lacked one or more aspects of the approach undertaken in this paper. The goal of this paper is to evaluate the performance of the proposed algorithm on several benchmark data sets and use the overall accuracy, sensitivity, specificity and F1 score to better quantify that evaluation. 


\section{Inspiration and Hypothesis}
\par The main idea that drove this research was the \textit{k}-means clustering approach not found in the literature review. Most of the review papers utilized one or the other measurement metric - either the within cluster distance or the intra-cluster distance. Several studies also explored the initialization problem of \textit{k}-means, some others looked at the selection of \textit{k} and the methods surrounding the most optimal value of \textit{k}. A number of recent research has revolved around the use of metaheuristics in clustering analysis. Some global optimization techniques like simulated annealing or evolutionary algorithms like genetic algorithm, particle swarm optimization have also been the center of \textit{k}-means clustering. With the novel algorithm presented in this project, the assumption is that by using both the within-cluster distance and inter-cluster distance, we could get a more robust idea on the performance of the algorithm as a whole. The hypothesis is that by including the ICD in the algorithm, we'll see higher accuracy, sensitivity, specificity and F1 score. 

