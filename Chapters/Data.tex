\section{Synthetic Data}
In order to quantify the proposed algorithm, a couple of synthetic data sets were generated from a normal distribution. One data set had a low variance to make the clustering more apparent and the other had a high variance. The idea with two variances was to generate one data set with more robust clusters and the other data set with more overlapping clusters. Having data sets with different variances would give a more accurate understanding on the performance of the algorithm. More importantly, it would allow for a more clear understanding of the performance of the algorithm against clustering that is not well defined. The intention was to visualize how the clustering changed and how the algorithm picked up outliers in the data set.
\par The first data set was generated with a variance of 0.5 to allow for a higher density within the clusters and the second one was generated with a variance of 1 for relatively ill-defined clusters. In each data set, there are a total of 3 clusters $(k = 3)$. In order to space out the clusters and define the coordinates around which the clusters are centered, a Kronecker tensor product was used to offset each cluster. A Kronecker product is defined as the product of two matrices of arbitrary sizes, resulting in a block matrix. It is denoted by $\otimes$.
Suppose $A$ is a $m \times n$ matrix and $B$ is a $p \times q$ matrix, then the Kronecker product, $A \otimes B$ is a resulting $mp \times nq$ block matrix \cite{31}. Mathematically,
$$ A \otimes B = 
\begin{bmatrix}
a_{11}B & \cdots & a_{1n}B \\
\vdots & \ddots & \vdots \\
a_{m1}B & \cdots & a_{mn}B
\end{bmatrix} $$

The first iteration of generating this synthetic data is run using a 2-dimensional data set and the next iteration is run using a 3-dimensional data set. This gives us a total of 4 synthetic data sets - Two 2-dimensional sets each with a variance of 0.5 and 1, two 3-dimensional sets each with a variance of 0.5 and 1.

\newpage

\section{UCI Machine Learning Repository}
The UCI Machine Learning Repository hosts a collection of databases and data sets that are used by researchers all over the world for the analysis of machine learning algorithms. The advantage of using some of these data sets hosted by the repository is the availability of labelled classes within the data set, which makes it easier to evaluate and quantify the algorithms using the pre-determined labels. As has been mentioned, the raw data in unsupervised learning has no labels and thus makes it hard to evaluate an algorithm against it. Clustering this data provides us no reference into the properties of the different clusters.
\par The first of this data set is the Iris data set. This data set is the most popular amongst researchers in the world of machine learning and pattern recognition. The set contains 150 data points (or instances) and four features (or attributes). The first attribute is the sepal length (in cm), the second is the sepal width (in cm), the third is the petal length (in cm) and lastly the petal width (in cm). The 150 instances are divided into 3 classes of 50 instances each. Each of the three classes represents a type of Iris flower. It is a perfect data set for clustering analysis and the benchmark set to analyze algorithms against \cite{32}. 
\par The second data set is the Wine data set which is a result of chemical analysis of wines that were grown in the same region of Italy but from three different cultivators. It has 178 instances and 13 attributes (Alcohol content, Malic acid content, Ash, Alcalinity of ash, Magnesium, Total phenols, Flavanoids, Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315 of diluted wines, Proline) that make up the data. The classification column contains the label of the three cultivators identified by 1, 2, or 3. There are 59 instances of Class 1, 78 instances of Class 2 and 48 instances of Class 3 \cite{32}. 
\par The third and final data set is the Wisconsin Breast Cancer data set that contains features computed from a digitized image of breast mass. The classification is based on whether an image shows a benign mass or a malignant mass (2 classes). It has a total of 569 instances and 9 attributes (not including the ID of the sample and the class) that help classify a data point as being malignant or benign. The features are: clump thickness, uniformity of cell size, uniformity of cell shape, marginal adhesion, single epithelial cell size, bare nuclei, bland chromatin, normal nucleoli, mitoses \cite{32}.