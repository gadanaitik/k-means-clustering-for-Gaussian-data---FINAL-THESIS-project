\par Machine learning can be broadly classified into three categories of algorithmic techniques depending on the type of problem being faced. These techniques also vary heavily, based on the learning algorithms themselves. They are: Supervised Learning, Reinforcement Learning and Unsupervised Learning. 

\par Supervised learning makes use of predetermined classes of the responses to train the underlying algorithm and make predictions based on the same. The features or the independent variables, \textbf{\textit{X}} are mapped to the responses or the dependent variables, \textbf{\textit{Y}}. The algorithm learns from this mapping to make predictions on unseen data. Hence the name, machine learning \cite{24}. It is the human equivalent of learning from our past experiences in order to gather knowledge that is imperative to improve our ability to perform future tasks in the real world. Supervised learning algorithms can either be categorized as regression or classification. Dependent variables with continuous data fall into the regression category whereas dependent variables with discrete data labels fall into the classification category \cite{22}. 

\par Reinforcement learning is another entity altogether wherein the algorithm interacts with the environment based on some actions, \textbf{\textit{A}} = ${a_1, a_2, ... a_N}$ and tries to produce an optimal policy or decision, by trial and error, based on rewards (or punishments), \textbf{\textit{R}} = ${r_1, r_2, ... r_N}$. The goal with this algorithm is to produce a decision based on a path that maximizes the rewards or minimize the punishments by the time the stopping criteria is met \cite{27}, \cite{29}.

\par Contrary to the supervised learning algorithms where the output data is labelled, unsupervised algorithms almost seem black boxed because there is no coherent knowledge about the data prior to processing it \cite{18}. The features or the independent variables, \textbf{\textit{X}} = ${x_1, x_2, .... x_N}$, where \textbf{\textit{X}} is an N-dimensional set of vectors, have no associated labels and are hence at the mercy of the processing algorithm to identify the homogeneity within the data set. This data could be customer information at a marketing research company, a set of handwritten alphabets or numbers, confidential patient data for the medical records, pixel information from an image. Unsupervised learning and specifically exploratory data analysis is now being widely used as the first step in gaining a deeper understanding of the raw data \cite{26}. One approach calls for the use of metaheuristic algorithms to deal with the NP-hardness of the unsupervised learning algorithms \cite{19}. Metaheuristic algorithms are high level guiding strategies that search the solution space for the most optimal solution. Despite being able to reduce the time complexity of finding a solution, metaheuristic algorithms have one major drawback: they are known to get stuck in the local optima \cite{19}, \cite{28}. Another approach calls for genetic algorithm to better search the solution space for optimized results \cite{11}. Genetic algorithms are a part of the \say{Evolutionary Algorithms} that mimic real life evolution to search for an optimized solution. Other evolutionary algorithms include Pattern Search, Particle Swarm Optimization, Simulated Annealing \cite{11}, \cite{12}, \cite{19}. Whatever the approach may be, in order to solve the unsupervised learning problems, we have to deploy methods that find patterns beyond the raw data which is nothing but unstructured noise. One method to understand the homogeneity in the underlying raw data is clustering analysis \cite{1}, \cite{23}, \cite{25}. 

\section{Clustering Analysis}
Clustering analysis is a class of unsupervised learning techniques that is the most widely researched \cite{1}, \cite{10}, \cite{21}. It garners more interest particularly because of its applications in deep learning and its potential ability to produce sophisticated artificial intelligence algorithms \cite{1}. Unsupervised learning algorithms, specifically clustering algorithms have been researched for the last four decades \cite{22}. The goal of clustering is to separate the unlabelled data into a finite set of \say{clusters} with similar properties among the data within the same clusters but differing properties from the data in other clusters \cite{2}, \cite{4}. Its applications include, but are not limited to data mining, image segmentation, vector quantization, pattern recognition, etc. \cite{2}. The definition of a \say{good} cluster remains implicit in that it depends on the application. There are many methods to classify these clusters, both ad hoc and systematic \cite{2}, \cite{4}, \cite{3}. Hierarchical clustering uses recursion to find clusters within the data by either a \say{top down} or a \say{bottom up} approach with a higher than quadratic complexity \cite{7}. This makes them highly unsuitable for larger data sets. In contrast, partitional algorithms find all the clusters within the data simultaneously and are computationally much cheaper than hierarchical algorithms making them favorable for large data sets. Clustering analysis is fundamental in deriving an insight into purely unstructured and noisy data. What sets the tone for further analysis of the clustered data is the similarity in properties of data belonging to the same clusters. These properties can either be distance based measures like Euclidean or Manhattan or they can be density based clustering like DBSCAN and BIRCH \cite{2}. Marketing research companies make use of clustering analysis to better target their audiences \cite{29}. Similarly even social media companies make use of clustering analysis and graph theory as a way to link similar social groups together and target ads based on the same. 
One such popular method of clustering is using a pre-determined number of clusters to classify the data into \textit{k} different clusters \cite{10}. \textit{k}-means clustering is undoubtedly one of the most popular clustering algorithms in existence today evident by the hundreds of publications on the topic in the last 30-40 years, each one extending and improving the \textit{k}-means algorithm in some way or another.

\newpage

\section{\textit{k}-means Clustering}
Given a set of \textit{p} data points in a \textit{d}-dimensional space, $R^d$, the goal of \textit{k}-means clustering is to populate the \textit{p} data points into \textit{k} clusters, where the value of \textit{k} is predetermined \cite{2}. The \say{grouping} of the data points is achieved in a way that minimizes the distance between the cluster centers and their corresponding data points. One of the biggest drawbacks of the \textit{k}-means clustering algorithm is that the number of clusters, \textit{k} have to be predetermined before the algorithm is applied. This may seem like a trivial step since the human eye is able to estimate the number of clusters when the data is first visualized. On the other hand, this task becomes difficult if the data is highly dimensional which makes it hard to visualize. It becomes even trickier to predetermine the number of clusters if the algorithm is to be deployed to multiple data sets at a time. The other major drawback of this algorithm is that it is sensitive to the initial position of the cluster centers which causes it to converge at local minima rather than the global optima \cite{5}, \cite{2}. Despite the uncertainty of achieving a global minima, \textit{k}-means clustering at least ensures convergence. As mentioned earlier, \textit{k}-means is a partitional algorithm which makes it computationally favorable to hierarchical clustering. Albeit as we will find out later in this paper it is not immune to data that is not normal or even extremely large data sets. In this paper, a simple step is proposed to increase the efficiency of the clustering and also to improve the overall accuracy of the algorithm. Instead of using just one distance measure to minimize the homogeneity within a cluster, we propose adding a distance measure to maximize the heterogeneity among different clusters. In other words, by increasing the inter-cluster distance (ICD) while decreasing the within-cluster distance (WCD), it can be hypothesized that the algorithm performs better.